{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# (1) === Fetch from Jina API ===\n",
    "JINA_API_KEY = \"jina_7f50a6d5bbbe45c6a75ff4dbfd946255mZMrkbGsQEH3dm8CONGY3yV6d1kv\"  # ← replace with your key\n",
    "api_url = \"https://api.jina.ai/v1/embeddings\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
    "}\n",
    "\n",
    "# We’ll compare for just one text + one image:\n",
    "single_payload = {\n",
    "    \"model\": \"jina-clip-v1\",\n",
    "    \"input\": [\n",
    "        {\"text\": \"A blue cat\"},\n",
    "        {\"image\": \"https://i.pinimg.com/600x315/21/48/7e/21487e8e0970dd366dafaed6ab25d8d8.jpg\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(api_url, headers=headers, json=single_payload)\n",
    "resp.raise_for_status()\n",
    "api_output = resp.json()\n",
    "\n",
    "# The API returns a list of embeddings (one per item in “input”).\n",
    "# We expect api_output[\"embeddings\"] to be e.g. [[…], […]] of size 2×dim\n",
    "api_text_emb_api = np.array(api_output[\"embeddings\"][0])\n",
    "api_img_emb_api  = np.array(api_output[\"embeddings\"][1])\n",
    "\n",
    "print(\"API embeddings shapes:\",\n",
    "      api_text_emb_api.shape, api_img_emb_api.shape)\n",
    "\n",
    "# (2) === Compute locally using transformers + jina-clip ===\n",
    "# Make sure you’ve already: pip install transformers einops timm pillow torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# Load the Jina-CLIP model & processor\n",
    "model = AutoModel.from_pretrained(\"jinaai/jina-clip-v1\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"jinaai/jina-clip-v1\")\n",
    "\n",
    "# Prepare text\n",
    "texts = [\"A blue cat\"]\n",
    "# Prepare image (will download behind the scenes via PIL)\n",
    "from PIL import Image\n",
    "import requests as _req\n",
    "from io import BytesIO\n",
    "\n",
    "img_url = \"https://i.pinimg.com/600x315/21/48/7e/21487e8e0970dd366dafaed6ab25d8d8.jpg\"\n",
    "img_data = Image.open(BytesIO(_req.get(img_url).content)).convert(\"RGB\")\n",
    "\n",
    "# Tokenize/process inputs\n",
    "inputs = processor(\n",
    "    text=texts, \n",
    "    images=[img_data], \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The model’s output has a ‘text_embeds’ and ‘image_embeds’ field\n",
    "# depending on Jina’s implementation. Check key names:\n",
    "if hasattr(outputs, \"text_embeds\") and hasattr(outputs, \"image_embeds\"):\n",
    "    txt_emb_loc = outputs.text_embeds.cpu().numpy()[0]\n",
    "    img_emb_loc = outputs.image_embeds.cpu().numpy()[0]\n",
    "else:\n",
    "    # Some CLIP variants name them differently; fallback:\n",
    "    txt_emb_loc = outputs.get(\"text_embeds\").cpu().numpy()[0]\n",
    "    img_emb_loc = outputs.get(\"image_embeds\").cpu().numpy()[0]\n",
    "\n",
    "print(\"Local embeddings shapes:\", txt_emb_loc.shape, img_emb_loc.shape)\n",
    "\n",
    "# (3) === Compare API vs. Local ===\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray):\n",
    "    a_norm = a / np.linalg.norm(a)\n",
    "    b_norm = b / np.linalg.norm(b)\n",
    "    return float(np.dot(a_norm, b_norm))\n",
    "\n",
    "# Cosine between text‐embeddings (API vs local)\n",
    "cos_text = cosine_similarity(api_text_emb_api, txt_emb_loc)\n",
    "\n",
    "# Cosine between image‐embeddings (API vs local)\n",
    "cos_img  = cosine_similarity(api_img_emb_api, img_emb_loc)\n",
    "\n",
    "# L2 norm differences\n",
    "l2_text = np.linalg.norm(api_text_emb_api - txt_emb_loc)\n",
    "l2_img  = np.linalg.norm(api_img_emb_api  - img_emb_loc)\n",
    "\n",
    "print(f\"\\nText embedding cosine (API vs Local): {cos_text:.6f}\")\n",
    "print(f\"Text embedding L2 difference     : {l2_text:.6f}\")\n",
    "\n",
    "print(f\"\\nImage embedding cosine (API vs Local): {cos_img:.6f}\")\n",
    "print(f\"Image embedding L2 difference       : {l2_img:.6f}\")\n",
    "\n",
    "# Decide if “same or not” within a tiny numerical tolerance:\n",
    "TOL = 1e-4\n",
    "print(\"\\n== SUMMARY ==\")\n",
    "print(\"Text embeddings match? \",\n",
    "      \"YES\" if l2_text < TOL else \"NO, Δ={:.6e}\".format(l2_text))\n",
    "print(\"Image embeddings match?\",\n",
    "      \"YES\" if l2_img < TOL else \"NO, Δ={:.6e}\".format(l2_img))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
